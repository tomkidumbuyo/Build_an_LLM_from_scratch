{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a236a3c-49b3-461b-9341-3dbd06bf5e97",
   "metadata": {},
   "source": [
    "# Writing Attention mechanisms\n",
    "The attention mechanism in large language models (LLMs) allows each token in a sequence to dynamically focus on the most relevant other tokens when building its representation, instead of relying only on fixed-size contexts like in traditional RNNs or CNNs. By computing similarity scores between queries, keys, and values derived from token embeddings, attention lets the model capture dependencies across arbitrary distances — for example, linking a pronoun to its noun several sentences earlier. This enables LLMs to model long-range context, disambiguate meaning, and integrate information from the whole sequence efficiently, which is the foundation of how transformers achieve state-of-the-art performance in natural language understanding and generation.\n",
    "\n",
    "We will implement four different variants of attention mechanisms These different attention variants build on each other.\n",
    "1. **Simplified self-attention**\n",
    "   A simplified self-attention technique to introduce the broader idea.\n",
    "3. **Self-attention**\n",
    "   Self-attention with trainable weights that forms the basis of the mechanism used in LLMs\n",
    "5. **Causal attention**\n",
    "   A type of self-attention used in LLMs that allows a model to consider only previous and current inputs in a sequence, ensuring temporal order during the text generation\n",
    "7. **Multi-head attention**\n",
    "   An extension of self-attention and causal attention that enables the model to simultaneously attend to information from different representation subspaces.\n",
    "\n",
    "Lets start with our data input. Consider we have a sentence <b>\"Your journey starts with one step\"</b>. This sentence has been tokenized and embeded, so we end up with the tensor we have bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024a6b38-81db-4ec8-9df2-6b4066fde671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4300, 0.1500, 0.8900])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    " [[0.43, 0.15, 0.89], # Your (x^1)\n",
    " [0.55, 0.87, 0.66], # journey (x^2)\n",
    " [0.57, 0.85, 0.64], # starts (x^3)\n",
    " [0.22, 0.58, 0.33], # with (x^4)\n",
    " [0.77, 0.25, 0.10], # one (x^5)\n",
    " [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afe48f-57d2-4ec0-b0ba-f32452d4d4a5",
   "metadata": {},
   "source": [
    "Lets start with a simplified self attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8fcff-8ea9-41e4-b7ac-5ae3cfe7323b",
   "metadata": {},
   "source": [
    "## 1. Attending to different parts of the input with self-attention\n",
    "We’ll now cover the inner workings of the self-attention mechanism and learn how to code it from the ground up. Self-attention serves as the cornerstone of every LLM based on the transformer architecture.\n",
    "\n",
    "### a. Simplified self-attention mechanism without trainable weights (we will just do this for 1 token)\n",
    "Let’s begin by implementing a simplified variant of self-attention, free from any trainable weights. The goal is to illustrate a few key concepts in self-attention before adding trainable weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f1c9b-9de5-4ee7-bf4e-ee6ee38caa19",
   "metadata": {},
   "source": [
    "The first step of implementing self-attention is to compute the intermediate values `ω`, referred to as attention scores. We calculate the intermediate attention scores between the query token and each input token. We determine these scores by computing the dot product of the query, `x(2)`, with every other input token. To demostrate this we have picked the second input embeding `[0.55, 0.87, 0.66]` for the word **\"journey\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a2ff9b-465a-4190-b125-32d38ac8a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "# this Creates a new uninitialized tensor of shape inputs.shape[0]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    # here we calculate the dot product of the input of the current iterataion of the input \n",
    "    # embeding tensor 'x_i' to our selected input \"input[1].\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "# we get a responce of a tensor with shape [1, 6]\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4cf7b-e708-46db-b89e-c621dae1942c",
   "metadata": {},
   "source": [
    "We got a result of a tensor of shape `[1, 6]`. This shows that we have we have 1 score for each embeding on our current sentence, hence in the end every word->token->embediding will have scores->weight for every other word in the sentence.\n",
    "<br>\n",
    "To convert scores to weights we need to normalize the output tensor. We use softmax for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc38f5ea-82aa-4f59-ae63-cc06e9a71b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b885a-709b-467b-993a-cdaf32f0e054",
   "metadata": {},
   "source": [
    "Now that we have computed the normalized attention **weights**, we are ready for the final step of calculating the **context** vector `z(2)` by multiplying the **embedded input tokens**, `x(i)`, with the corresponding attention weights and then summing the resulting vectors. Thus, context vector `z(2)` is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724c6da3-93c7-4cbe-9c20-c1cf52b3dfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a5796-056b-46d3-a8b9-cb68aed171f7",
   "metadata": {},
   "source": [
    "** Summary of Self-Attention **\n",
    "- Compute scores: Each token compares itself to all other tokens using a similarity measure (dot product).\n",
    "- Normalize scores: Apply softmax to convert scores into attention weights that sum to 1.\n",
    "- Compute context vectors: Multiply the attention weights with the value vectors to get a new representation for each token. The context vectors have the same shape as the original embeddings.\n",
    "\n",
    "```\n",
    "Token Embeddings (X)\n",
    "   ┌───────────────┐\n",
    "   │  x1  x2  x3   │  <-- shape [T, d_model]\n",
    "   └───────────────┘\n",
    "           │\n",
    "           ▼\n",
    "Compute Scores (Q · K^T / √d)\n",
    "   ┌───────────────┐\n",
    "   │ s11 s12 s13   │\n",
    "   │ s21 s22 s23   │  <-- shape [T, T]\n",
    "   │ s31 s32 s33   │\n",
    "   └───────────────┘\n",
    "           │\n",
    "           ▼\n",
    "Softmax to get Attention Weights (α)\n",
    "   ┌───────────────┐\n",
    "   │ α11 α12 α13   │\n",
    "   │ α21 α22 α23   │  <-- rows sum to 1\n",
    "   │ α31 α32 α33   │\n",
    "   └───────────────┘\n",
    "           │\n",
    "           ▼\n",
    "Weighted sum of Values to get Context Vectors (C)\n",
    "   ┌───────────────┐\n",
    "   │ c1  c2  c3    │  <-- shape [T, d_model], same as embeddings\n",
    "   │ c1  c2  c3    │\n",
    "   │ c1  c2  c3    │\n",
    "   └───────────────┘\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b78e8-4458-4c9a-ab87-c1cad1a8c9ec",
   "metadata": {},
   "source": [
    "### b. Computing attention weights for all input tokens\n",
    "So far, we have computed attention weights and the context vector for input 2 (`input[1]`). Now let’s extend this computation to calculate attention weights and context vectors for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c27a3dbb-007c-4854-8cc5-6bf74c4eb7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8194e-851e-450f-b650-1c6496ad8934",
   "metadata": {},
   "source": [
    "Each element in the tensor represents an attention score between each pair of inputs. Note that the values in that figure are normalized, which is\n",
    "why they differ from the unnormalized attention scores in the preceding tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05adb4bb-80b6-43c3-8436-58ae648be007",
   "metadata": {},
   "source": [
    "When computing the preceding attention score tensor, we used for loops in Python. However, for loops are generally slow, and we can achieve the same results using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf3f33c-c154-4048-a57f-fef8d2b88ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68685be6-2997-40e4-9e45-6aa6143ad6a2",
   "metadata": {},
   "source": [
    "Then we normalize each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d936c624-39c8-4115-9c19-0cbc9d22a28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c52aee-3240-434d-a8ff-7e36bfe8c64a",
   "metadata": {},
   "source": [
    "In the context of using PyTorch, the dim parameter in functions like `torch.softmax` specifies the dimension of the input tensor along which the function will be computed. By setting dim=-1, we are instructing the softmax function to apply the normalization along the last dimension of the attn_scores tensor. If attn_scores is a two-dimensional tensor (for example, with a shape of [rows, columns]), it will normalize across the columns so that the values in each row (summing over the column dimension) sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b112c9c3-a37e-4c8b-9c06-6cb24abd7d5f",
   "metadata": {},
   "source": [
    "In the third and final step, we use these attention weights to compute all context vectors via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8b7dcd-e8a0-47e5-adbb-eaef7c8efead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93461e-1173-44fa-95f2-2d85e5d3e4e1",
   "metadata": {},
   "source": [
    "## 2. Implementing self-attention with trainable weights\n",
    "Our next step will be to implement the self-attention mechanism used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called scaled dot-product attention. The self-attention mechanism with trainable weights builds on the previous concepts. We want to compute context vectors as weighted sums over the input vectors specific to a certain input element.<br>\n",
    "We will implement the self-attention mechanism step by step by introducing thethree trainable weight matrices \n",
    "- Wq (Query weight matrix): A learned projection that transforms the input embeddings into queries, representing what each token is looking for.\n",
    "- Wk (Key weight matrix): A learned projection that transforms the input embeddings into keys, representing what each token has to offer for matching.\n",
    "- Wv (Value weight matrix): A learned projection that transforms the input embeddings into values, representing the actual content or information to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5ecf1-c438-4ad9-af0a-fb0c241e1902",
   "metadata": {},
   "source": [
    "### a. Computing the attention weights step by step\n",
    "Earlier, we defined the second input element `x(2)` as the query when we computed the simplified attention weights to compute the context vector `z(2)`. Then we generalized this to compute all context vectors `z(1) ... z(T)` for the six-word input sentence **“Your journey starts with one step.”** Similarly, we start here by computing only one context vector, `z(2)`, for illustration purposes. We will then modify this code to calculate all context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a19a22-c1c5-4b95-a9c5-adb39facfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd9115-80fc-46fe-8a30-e82a17f2efa4",
   "metadata": {},
   "source": [
    "Note that in GPT-like models, the input and output dimensions are usually the same, but to better follow the computation, we’ll use different input `(d_in=3)` and output `(d_out=2)` dimensions here.<br><br>\n",
    " <b>Next</b>, we initialize the three weight matrices Wq, Wk, and Wv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7ef5ea-f05c-4165-a800-d90e187b438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae575bd2-41e5-481d-9b09-5eda5eae5c61",
   "metadata": {},
   "source": [
    "We set `requires_grad=False` to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would set `requires_grad=True` to update these matrices during model training.<br><br>\n",
    " <b>Next</b>, we compute the query, key, and value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcdbed19-be6e-43c9-8df8-680aa1f3f8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query tensor([0.4306, 1.4551])\n",
      "key tensor([0.4433, 1.1419])\n",
      "value tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print('query', query_2)\n",
    "print('key', key_2)\n",
    "print('value', value_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c995fff-a2bb-4bde-b02a-c06ebd8f4713",
   "metadata": {},
   "source": [
    "Even though our temporary goal is only to compute the one context vector, `z(2)`, we still require the key and value vectors for all input elements as they are involved in computing the attention weights with respect to the query `q(2)`. We can obtain all keys and values via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39edcbad-8b3e-41f1-820f-bc49a5293c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7519c-af32-47e5-918e-303e324fcd51",
   "metadata": {},
   "source": [
    "First, let’s compute the attention score ω22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db4913ef-d592-42aa-b5e7-ff2c3eee1c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468f10e-b1c9-4861-9c49-e57ebd7c9248",
   "metadata": {},
   "source": [
    "Again, we can generalize this computation to all attention scores via matrix\n",
    "multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f75b04a3-c9b0-4891-a76c-e011d2e92369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f929b6-670d-4699-9be7-fafa4beb3060",
   "metadata": {},
   "source": [
    "Now, we want to go from the attention scores to the attention weights. We compute the attention weights by scaling the attention scores and using the softmax function. However, now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (taking the square root is mathematically the same as exponentiating by 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b96c83c-8068-43de-a62a-29ce4f8cab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ce5a5-d0ad-4e4c-b929-b44815072573",
   "metadata": {},
   "source": [
    "Finally we will compute the context vector as a weighted sum over the value vectors as we have done before. The attention weights serve as a weighting factor that weighs the respective importance of each value vector. Also as before, we can use matrix multiplication to obtain the output in one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "922ce926-85ea-4815-8fe5-b13c997f0ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcbb2b-d448-4f90-a238-c61b2e37c172",
   "metadata": {},
   "source": [
    "### b. Implementing a compact self-attention Python class\n",
    "At this point, we have gone through a lot of steps to compute the self-attention outputs. We did so mainly for illustration purposes so we could go through one step at a time. In practice, with the LLM implementation in the next chapter in mind, it is helpful to organize this code into a Python class, as shown in the following listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "599f93a7-b205-4b77-a90b-c1cc1359c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9bcc1a-13a1-40ce-a2a4-f962b29aff76",
   "metadata": {},
   "source": [
    "We create `SelfAttention_v1` class inherited from `nn.Module`, which is a fundamental building block of PyTorch models that provides necessary model layer creation and management.\n",
    " The `__init__` method initializes trainable weight matrices **(W_query, W_key, and W_value)** for queries, keys, and values, each transforming the input dimension `d_in` to an output dimension `d_out`. During the forward pass, using the forward method, we compute the attention scores `(attn_scores)` by multiplying queries and keys, normalizing these scores using `softmax`. Finally, we create a context vector by weighting the values with these normalized attention scores.<br><br>\n",
    "We can use this class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c3ed30d-ba0d-4625-8537-db42cd2b066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4146c94-00b1-4eda-9015-14a8f6805cbe",
   "metadata": {},
   "source": [
    "Since inputs contains six **embedding vectors**, this results in a matrix storing the six **context vectors**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed9827-f076-4445-9fcf-fdff70b7cf88",
   "metadata": {},
   "source": [
    " Self-attention involves the trainable weight matrices Wq, Wk, and Wv. These matrices transform input data into queries, keys, and values, respectively, which are crucial components of the attention mechanism. As the model is exposed to more data during training, it adjusts these trainable weights, as we will see in upcoming chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44e531-44b7-48e6-a635-2613e82999a7",
   "metadata": {},
   "source": [
    "### c. A self-attention class using PyTorch’s Linear layers\n",
    " We can improve the `SelfAttention_v1` by using fully connected (dense) layer `nn.Linear` layers, which effectively perform matrix multiplication when\n",
    "the bias units are disabled. Additionally, a significant advantage of using `nn.Linear` instead of manually implementing `nn.Parameter(torch.rand(...))` is that `nn.Linear` has an optimized weight initialization scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ad4145c-95d3-469d-88ca-23f3dd885d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1849799-0d71-45d2-a3bc-ee57a42086c1",
   "metadata": {},
   "source": [
    "You can use the `SelfAttention_v2` similar to `SelfAttention_v1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff4fa2dd-344a-4be4-a778-2f7f7a2a7b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa2e0c-2fb1-44f0-b80f-c4dc345bcc8f",
   "metadata": {},
   "source": [
    "## 3. Hiding future words with Causal attention\n",
    "\n",
    "Wel want the self-attention mechanism to consider only the tokens that appear prior to the current position when predicting the next token in a sequence. Causal attention, also known as masked attention, is a specialized form of selfattention. It restricts a model to only consider previous and current inputs in a sequence. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.<br>\n",
    "To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text, as illustrated bellow: \n",
    "\n",
    "| Query \\ Key | your | journey | starts | with | one | step |\n",
    "| ----------- | ---- | ------- | ------ | ---- | --- | ---- |\n",
    "| your        | ✓    | <span style=\"color:red\">✗</span>       | <span style=\"color:red\">✗</span>      | <span style=\"color:red\">✗</span>    | <span style=\"color:red\">✗</span>   | <span style=\"color:red\">✗</span>    |\n",
    "| journey     | ✓    | ✓       | <span style=\"color:red\">✗ </span>     | <span style=\"color:red\">✗</span>    | <span style=\"color:red\">✗</span>   | <span style=\"color:red\">✗</span>    |\n",
    "| starts      | ✓    | ✓       | ✓      | <span style=\"color:red\">✗</span>    | <span style=\"color:red\">✗</span>   | <span style=\"color:red\">✗</span>    |\n",
    "| with        | ✓    | ✓       | ✓      | ✓    | <span style=\"color:red\">✗   | <span style=\"color:red\">✗</span>    |\n",
    "| one         | ✓    | ✓       | ✓      | ✓    | ✓   | <span style=\"color:red\">✗</span>    |\n",
    "| step        | ✓    | ✓       | ✓      | ✓    | ✓   | ✓    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712feaf-697a-4a47-89d0-2b2b7da15098",
   "metadata": {},
   "source": [
    "### a. Applying a causal attention mask\n",
    "To implement the apply a causal attention mask to obtain the masked attention weights, let’s work with the attention scores and weights from the previous section to code the causal attention mechanism. \n",
    "In the first step, we compute the attention weights using the softmax function as we have done previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d306aa5-06b9-40db-8204-d3ee9ce11d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392529ab-626c-4140-b38a-35c4ef15a87b",
   "metadata": {},
   "source": [
    "We can implement the second step using PyTorch’s tril function to create a mask\n",
    "where the values above the diagonal are zero:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29b5f17d-0c0f-43ae-bb22-aa4d54cf00ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ee04f-5f71-482f-aa7f-c930a6c28c7c",
   "metadata": {},
   "source": [
    "Now, we can multiply this mask with the attention weights to zero-out the values above\n",
    "the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d10f9c83-92ed-4479-9d02-b694397c90aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8204f-e24f-46aa-94fa-14040e8682f9",
   "metadata": {},
   "source": [
    "The third step is to renormalize the attention weights to sum up to 1 again in each\n",
    "row. We can achieve this by dividing each element in each row by the sum in each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ab81e07-5bf9-4ce8-afae-4c0e50f90895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d0cea-1012-4c6b-8812-e8982887e7e0",
   "metadata": {},
   "source": [
    "Let’s take a mathematical property of the softmax function and implement the computation of the masked attention weights more efficiently in fewer steps. The softmax function converts its inputs into a probability distribution. When negative infinity values (-∞) are present in a row, the softmax function treats them as zero probability. (Mathematically, this is because e –∞ approaches 0.)<br>\n",
    " We can implement this more efficient masking “trick” by creating a mask with 1s\n",
    "above the diagonal and then replacing these 1s with negative infinity (-inf) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "456e1289-2b7d-4b8a-84c8-ef03915ab642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ca5e1-889f-4fd3-bd20-c6f19d31df36",
   "metadata": {},
   "source": [
    "Now all we need to do is apply the softmax function to these masked results, and we\n",
    "are done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a56f8f7c-1847-4982-b5f2-98ab96936323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70171e3d-e146-46c6-b15a-c0aba74313b7",
   "metadata": {},
   "source": [
    "### b. Masking additional attention weights with dropout\n",
    "Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively “dropping” them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It’s important to emphasize that dropout is only used during training and is disabled afterward.<br>\n",
    " In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors. Here we will apply the dropout mask after computing the attention weights, because it’s the more common variant in practice.\n",
    " In the following code example, we use a dropout rate of 50%, which means masking out half of the attention weights. (When we train the GPT model,\n",
    "we will use a lower dropout rate, such as 0.1 or 0.2.)<br> 4\n",
    "As a demostration We have applied PyTorch’s dropout implementation first to a `6 × 6` tensor consisting of 1s for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d41861f-f0cc-46a3-9d6e-27de461c3048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 x 6 matrix before dropout: \n",
      " tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "6 x 6 matrix after dropout: \n",
      " tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(\"6 x 6 matrix before dropout: \\n\", example)\n",
    "print(\"6 x 6 matrix after dropout: \\n\", dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4136870-c1ce-46eb-82a2-b2ea26d074c0",
   "metadata": {},
   "source": [
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 = 2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases.<br>\n",
    " Now let’s apply dropout to the attention weight matrix itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7b3b807-11a9-4fa5-9fd9-82b19586923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da89447-d224-4c58-aa8e-6dc4417eeb16",
   "metadata": {},
   "source": [
    "### c. Implementing a compact causal attention class\n",
    "We will now incorporate the causal attention and dropout modifications into the\n",
    "SelfAttention Python class we developed in section 3.4. This class will then serve as a\n",
    "template for developing multi-head attention, which is the final attention class we will\n",
    "implement.\n",
    " But before we begin, let’s ensure that the code can handle batches consisting of\n",
    "more than one input so that the CausalAttention class supports the batch outputs\n",
    "produced by the data loader we implemented in chapter 2.\n",
    " For simplicity, to simulate such batch inputs, we duplicate the input text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83532796-9460-40eb-845f-5bd194d90828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d57734-439c-4288-b27b-7e2cf5f5a2c7",
   "metadata": {},
   "source": [
    "The following CausalAttention class is similar to the SelfAttention class we implemented earlier, except that we added the dropout and causal mask components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3d543d5-9bc2-4e9f-8a56-6ac6c850331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "         super().__init__()\n",
    "         self.d_out = d_out\n",
    "         self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "         self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "         self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "         self.dropout = nn.Dropout(dropout)\n",
    "         self.register_buffer(\n",
    "             'mask',\n",
    "             torch.triu(torch.ones(context_length, context_length),\n",
    "             diagonal=1)\n",
    "         )\n",
    "        \n",
    "    def forward(self, x):\n",
    "         b, num_tokens, d_in = x.shape\n",
    "         keys = self.W_key(x)\n",
    "         queries = self.W_query(x)\n",
    "         values = self.W_value(x)\n",
    "         attn_scores = queries @ keys.transpose(1, 2)\n",
    "         attn_scores.masked_fill_(\n",
    "         self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "         attn_weights = torch.softmax(\n",
    "             attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "         )\n",
    "         attn_weights = self.dropout(attn_weights)\n",
    "         context_vec = attn_weights @ values\n",
    "         return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ca35d-6079-411d-8717-af5ea6c6e697",
   "metadata": {},
   "source": [
    "While all added code lines should be familiar at this point, we now added a `self.register_buffer()` call in the `__init__` method. The use of register_buffer in PyTorch is not strictly necessary for all use cases but offers several advantages here. For instance, when we use the CausalAttention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will\n",
    "be relevant when training our LLM. This means we don’t need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors.<br><br>\n",
    "We can use the CausalAttention class as follows, similar to SelfAttention previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ce44f5e-ec43-48f8-911f-f02152dd7165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba45f15-eaca-4754-9f18-b37052404de1",
   "metadata": {},
   "source": [
    "## 4. Extending single-head attention to multi-head attention\n",
    "Our final step will be to extend the previously implemented causal attention class over <b>multiple heads</b>. This is also called <b>multi-head attention.</b><br><br>\n",
    "    The term “multi-head” refers to dividing the attention mechanism into multiple “heads,” each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.<br><br>\n",
    "    We will tackle this expansion from causal attention to multi-head attention. First, we will intuitively build a multi-head attention module by stacking multiple CausalAttention modules. Then we will then implement the same multi-head attention module in a more complicated but more computationally efficient way.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edfd4b-9756-4b10-971b-b0c9da85bfa9",
   "metadata": {},
   "source": [
    "### a. Stacking multiple single-head attention layers\n",
    "In practical terms, implementing multi-head attention involves creating multiple instances of the self-attention mechanism, each with its own weights,\n",
    "and then combining their outputs. this is important for the kind accuracy we are going for.<br><br>\n",
    "  \n",
    "This will  run the attention mechanism multiple times (in parallel). Here is a simple MultiHeadAttentionWrapper class that stacks multiple instances of our previously implemented CausalAttention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ccab7e8-b47c-42db-8aea-dac750881a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "         super().__init__()\n",
    "         self.heads = nn.ModuleList(\n",
    "             [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
    "         )\n",
    "         \n",
    "     def forward(self, x):\n",
    "         return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a904374a-7998-4edb-9639-2f767aa8c9c3",
   "metadata": {},
   "source": [
    "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via `num_heads=2`) and CausalAttention output dimension d_out=2, we get a fourdimensional context vector (`d_out*num_heads=4`). To illustrate this further with a concrete example, we can use the MultiHeadAttentionWrapper class similar to the CausalAttention class before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b56d50a1-c93d-43a5-9c4b-17d7d7ecad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "     d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6a7b2-0d11-4cf2-ab53-a01bad2233c3",
   "metadata": {},
   "source": [
    "We can combine `CausalAttention` and `MultiHeadAttentionWrapper`s into a single `MultiHeadAttention` class. Also, in addition to merging the `MultiHeadAttentionWrapper` with the `CausalAttention` code, we will make some other modifications to implement multi-head attention more efficiently.<br>\n",
    " In the `MultiHeadAttentionWrapper`, multiple heads are implemented by creating a list of CausalAttention objects `(self.heads)`, each representing a separate attention head. The `CausalAttention` class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following `MultiHeadAttention` class integrates the multi-head functionality within a single class.<br>\n",
    "It splits the input into multiple heads by reshaping the projected `query`, `key`, and `value` tensors and then combines the results from these heads after computing attention.<br><br>\n",
    "Let’s take a look at the MultiHeadAttention clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00f53ebb-0b49-4307-9fbd-e0aa01a7fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "         super().__init__()\n",
    "         assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "         self.d_out = d_out\n",
    "         self.num_heads = num_heads\n",
    "         self.head_dim = d_out // num_heads # Reduces the projection dim to match the desired output dim\n",
    "         \n",
    "         self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "         self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "         self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "         # Uses a Linear layer to combine head outputs\n",
    "         self.out_proj = nn.Linear(d_out, d_out)\n",
    "         self.dropout = nn.Dropout(dropout)\n",
    "         self.register_buffer(\n",
    "             \"mask\",\n",
    "             torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "         )\n",
    "         \n",
    "     def forward(self, x):\n",
    "         b, num_tokens, d_in = x.shape\n",
    "\n",
    "         # Tensor shape: (b, num_tokens, d_out)\n",
    "         keys = self.W_key(x)\n",
    "         queries = self.W_query(x)\n",
    "         values = self.W_value(x)\n",
    "\n",
    "         # We implicitly split the matrix by adding a num_heads dimension.  \n",
    "         # Then we unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "         keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "         values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "         queries = queries.view(\n",
    "             b, num_tokens, self.num_heads, self.head_dim\n",
    "         )\n",
    "\n",
    "         # Transposes from shape (b, num_tokens, num_heads, head_dim) \n",
    "         # to (b, num_heads, num_tokens, head_dim)\n",
    "         keys = keys.transpose(1, 2)\n",
    "         queries = queries.transpose(1, 2)\n",
    "         values = values.transpose(1, 2)\n",
    "\n",
    "         attn_scores = queries @ keys.transpose(2, 3) # Computes dot product for each head\n",
    "         mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # Masks truncated to the number of tokens\n",
    "        \n",
    "         attn_scores.masked_fill_(mask_bool, -torch.inf) # Uses the mask to fill attention scores\n",
    "         \n",
    "         attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "         attn_weights = self.dropout(attn_weights)\n",
    "         \n",
    "         context_vec = (attn_weights @ values).transpose(1, 2) # Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "         context_vec = context_vec.contiguous().view(\n",
    "             b, num_tokens, self.d_out\n",
    "         ) # Combines heads, where self.d_out = self.num_heads * self.head_dim\n",
    "         \n",
    "         context_vec = self.out_proj(context_vec) # Adds an optional linear projection\n",
    "         return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd6789-9bad-4b44-952b-0b6c00c1f753",
   "metadata": {},
   "source": [
    "Even though the reshaping (`.view`) and transposing (`.transpose`) of tensors inside the `MultiHeadAttention` class looks very mathematically complicated, the `MultiHeadAttention` class implements the same concept as the `MultiHeadAttentionWrappe`r earlier.<br>\n",
    "\n",
    " On a big-picture level, in the previous `MultiHeadAttentionWrapper`, we stacked multiple single-head attention layers that we combined into a multi-head attention layer. The `MultiHeadAttention` class takes an integrated approach. It starts with a multi-head layer and then internally splits this layer into individual attention heads.<br>\n",
    " \n",
    " The splitting of the query, key, and value tensors is achieved through tensor reshaping and transposing operations using PyTorch’s `.view` and `.transpose` methods. The input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads.<br>\n",
    " \n",
    " The key operation is to split the `d_out` dimension into `num_head`s and `head_dim`, where `head_dim = d_out / num_heads`. This splitting is then achieved using the `.view` method: a tensor of dimensions `(b, num_tokens, d_out)` is reshaped to dimension `(b, num_tokens, num_heads, head_dim)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c9b0b5-dcf7-44f5-976b-ddd8eb6ca4be",
   "metadata": {},
   "source": [
    "The tensors are then transposed to bring the `num_heads` dimension before the `num_tokens` dimension, resulting in a shape of `(b, num_heads, num_tokens, head_dim)`. This transposition is crucial for correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications efficiently. <br>\n",
    "To illustrate this batched matrix multiplication, suppose we have the tensor below, Then perform a batched matrix multiplication between the tensor itself and a view of the tensor where we transposed the last two dimensions, `num_tokens` and `head_dim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf1ff71a-8718-4cf4-a1d8-e7741ac25bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    " [0.8993, 0.0390, 0.9268, 0.7388],\n",
    " [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    " [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    " [0.4066, 0.2318, 0.4545, 0.9737],\n",
    " [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "print(a @ a.transpose(2, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cff464-a65b-4e45-b71d-10e38da4d14f",
   "metadata": {},
   "source": [
    "In this case, the matrix multiplication implementation in PyTorch handles the fourdimensional input tensor so that the matrix multiplication is carried out between the two\n",
    "last dimensions (num_tokens, head_dim) and then repeated for the individual heads.\n",
    " For instance, the preceding becomes a more compact way to compute the matrix\n",
    "multiplication for each head separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64dfc7d2-b184-4da0-afc5-a5ff924cc617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1972661-309f-433f-a383-d47793bca04c",
   "metadata": {},
   "source": [
    "The results are exactly the same results as those we obtained when using the batched\n",
    "matrix multiplication `print(a @ a.transpose(2, 3))`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b22d5-ae2f-463e-894c-4fb2f25e95fb",
   "metadata": {},
   "source": [
    "Continuing with MultiHeadAttention, after computing the attention weights and context vectors, the context vectors from all heads are transposed back to the shape `(b, num_tokens, num_heads, head_dim)`. These vectors are then reshaped (flattened) into the shape `(b, num_tokens, d_out)`, effectively combining the outputs from all heads.<br>\n",
    " Additionally, we added an output projection layer (`self.out_proj`) to `MultiHeadAttention` after combining the heads, which is not present in the `CausalAttention` class. This output projection layer is not strictly necessary, but it is commonly used in many LLM architectures, which is why I\n",
    "added it here for completeness.\n",
    " Even though the MultiHeadAttention class looks more complicated than the MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors, it is more efficient. The reason is that we only need one matrix multiplication to compute the keys, for instance, `keys = self.W_key(x)` (the same is true for the queries and values). In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication, which is computationally one of the most expensive steps, for each\n",
    "attention head.<br>\n",
    " The MultiHeadAttention class can be used similar to the SelfAttention and CausalAttention classes we implemented earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9498df3-2f8b-48f6-a3e2-98a9573bc07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab1d9b-dd12-4469-a94a-6cbda62f1102",
   "metadata": {},
   "source": [
    "We have now implemented the MultiHeadAttention class that we will use when we\n",
    "implement and train the LLM. Note that while the code is fully functional, I used\n",
    "relatively small embedding sizes and numbers of attention heads to keep the outputs\n",
    "readable.\n",
    " For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5\n",
    "billion parameters) has 25 attention heads and a context vector embedding size of\n",
    "1,600. The embedding sizes of the token inputs and context embeddings are the same\n",
    "in GPT models (d_in = d_out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a703425d-fa18-4a98-8266-306f911d429c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
